{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Practical Machine Learning Project\"\nauthor: \"Armand R.\"\ndate: \"November, 2016\"\noutput: html_document\n---\n## Prediction Assignment\n\n### Background\nUsing devices such as JawboneUp, NikeFuelBand, and Fitbitit is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in\ntheir behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.  \n   \nIn this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).   \n\n### Preparing the data and R packages  \n\n#### Load packages, set caching \n\n```{r, message=FALSE}\nrequire(caret)\nrequire(corrplot)\nrequire(Rtsne)\nrequire(xgboost)\nrequire(stats)\nrequire(knitr)\nrequire(ggplot2)\nrequire(e1071)\nknitr::opts_chunk$set(cache=TRUE)\n```\nFor fast and accurate training the model, I choose XGBoost, an implementation of tree-based extreme gradient boosting algorithm. (*As discussed in the course's forum, this XGBoost tool is confirmed by course's CTA to be allowed to be used in this assignment project.*)   \n\n#### Getting Data\n```{r}\n# URL of the training and testing data\ntrain.url =\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\ntest.url = \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\n# file names\ntrain.name = \"./data/pml-training.csv\"\ntest.name = \"./data/pml-testing.csv\"\n# if directory does not exist, create new\nif (!file.exists(\"./data\")) {\n  dir.create(\"./data\")\n}\n# if files does not exist, download the files\nif (!file.exists(train.name)) {\n  download.file(train.url, destfile=train.name, method=\"curl\")\n}\nif (!file.exists(test.name)) {\n  download.file(test.url, destfile=test.name, method=\"curl\")\n}\n# load the CSV files as data.frame \ntrain = read.csv(\"./data/pml-training.csv\")\ntest = read.csv(\"./data/pml-testing.csv\")\ndim(train)\ndim(test)\nnames(train)\n```  \n\nThe raw training data has 19622 rows of observations and 158 features (predictors). Column `X` is unusable row number. While the testing data has 20 rows and the same 158 features. There is one column of target outcome named `classe`.   \n\n#### Data cleaning\n\nFirst, extract target outcome (the activity quality) from training data, so now the training data contains only the predictors (the activity monitors).   \n```{r}\n# target outcome (label)\noutcome.org = train[, \"classe\"]\noutcome = outcome.org \nlevels(outcome)\n```\nOutcome has 5 levels in character format.   \nConvert the outcome to numeric, because XGBoost gradient booster only recognizes numeric data.   \n```{r}\n# convert character levels to numeric\nnum.class = length(levels(outcome))\nlevels(outcome) = 1:num.class\nhead(outcome)\n```\n   \nThe outcome is removed from training data.   \n```{r}\n# remove outcome from train\ntrain$classe = NULL\n```\n\nThe assignment rubric asks to use data from accelerometers on the `belt`, `forearm`, `arm`, and `dumbell`, so the features are extracted based on these keywords.   \n  \n```{r}\n# filter columns on: belt, forearm, arm, dumbell\nfilter = grepl(\"belt|arm|dumbell\", names(train))\ntrain = train[, filter]\ntest = test[, filter]\n```\n\nInstead of less-accurate imputation of missing data, remove all columns with NA values.   \n```{r}\n# remove columns with NA, use test data as referal for NA\ncols.without.na = colSums(is.na(test)) == 0\ntrain = train[, cols.without.na]\ntest = test[, cols.without.na]\n```\n\n### Preprocessing  \n\n#### Check for features's variance\n\nBased on the principal component analysis PCA, it is important that features have maximum variance for maximum uniqueness, so that each feature is as distant as possible (as orthogonal as possible) from the other features.   \n```{r}\n# check for zero variance\nzero.var = nearZeroVar(train, saveMetrics=TRUE)\nzero.var\n```\nThere is no features without variability (all has enough variance). So there is no feature to be removed further.  \n\n#### Plot of relationship between features and outcome  \n\nPlot the relationship between features and outcome. From the plot below, each features has relatively the same distribution among the 5 outcome levels (A, B, C, D, E).   \n```{r fig.width=12, fig.height=8, dpi=72}\nfeaturePlot(train, outcome.org, \"strip\")\n```\n\n#### Plot of correlation matrix  \n\nPlot a correlation matrix between features.   \nA good set of features is when they are highly uncorrelated (orthogonal) each others. The plot below shows average of correlation is not too high, so I choose to not perform further PCA preprocessing.   \n```{r fig.width=12, fig.height=12, dpi=72}\ncorrplot.mixed(cor(train), lower=\"circle\", upper=\"color\", \n               tl.pos=\"lt\", diag=\"n\", order=\"hclust\", hclust.method=\"complete\")\n```\n\n#### tSNE plot \n\nA tSNE (t-Distributed Stochastic Neighbor Embedding) visualization is 2D plot of multidimensional features, that is multidimensional reduction into 2D plane. In the tSNE plot below there is no clear separation of clustering of the 5 levels of outcome (A, B, C, D, E). So it hardly gets conclusion for manually building any regression equation from the irregularity.   \n\n```{r fig.width=12, fig.height=8, dpi=72}\n# t-Distributed Stochastic Neighbor Embedding\ntsne = Rtsne(as.matrix(train), check_duplicates=FALSE, pca=TRUE, \n              perplexity=30, theta=0.5, dims=2)\nembedding = as.data.frame(tsne$Y)\nembedding$Class = outcome.org\ng = ggplot(embedding, aes(x=V1, y=V2, color=Class)) +\n  geom_point(size=1.25) +\n  guides(colour=guide_legend(override.aes=list(size=6))) +\n  xlab(\"\") + ylab(\"\") +\n  ggtitle(\"t-SNE 2D Embedding of 'Classe' Outcome\") +\n  theme_light(base_size=20) +\n  theme(axis.text.x=element_blank(),\n        axis.text.y=element_blank())\nprint(g)\n```\n\n### Build machine learning model \n\nNow build a machine learning model to predict activity quality (`classe` outcome) from the activity monitors (the features or predictors) by using XGBoost extreme gradient boosting algorithm.    \n\n#### XGBoost data\n\nXGBoost supports only numeric matrix data. Converting all training, testing and outcome data to matrix.  \n\n```{r}\n# convert data to matrix\ntrain.matrix = as.matrix(train)\nmode(train.matrix) = \"numeric\"\ntest.matrix = as.matrix(test)\nmode(test.matrix) = \"numeric\"\n# convert outcome from factor to numeric matrix \n#   xgboost takes multi-labels in [0, numOfClass)\ny = as.matrix(as.integer(outcome)-1)\n```\n\n#### XGBoost parameters \n\nSet XGBoost parameters for cross validation and training.  \nSet a multiclass classification objective as the gradient boosting's learning function.   \nSet evaluation metric to `merror`, multiclass error rate.   \n\n```{r}\n# xgboost parameters\nparam <- list(\"objective\" = \"multi:softprob\",    # multiclass classification \n              \"num_class\" = num.class,    # number of classes \n              \"eval_metric\" = \"merror\",    # evaluation metric \n              \"nthread\" = 8,   # number of threads to be used \n              \"max_depth\" = 16,    # maximum depth of tree \n              \"eta\" = 0.3,    # step size shrinkage \n              \"gamma\" = 0,    # minimum loss reduction \n              \"subsample\" = 1,    # part of data instances to grow tree \n              \"colsample_bytree\" = 1,  # subsample ratio of columns when constructing each tree \n              \"min_child_weight\" = 12  # minimum sum of instance weight needed in a child \n              )\n```\n\n#### Expected error rate \n\nExpected error rate is less than `1%` for a good classification. Do cross validation to estimate the error rate using 4-fold cross validation, with 200 epochs to reach the expected error rate of less than `1%`.  \n\n#### 4-fold cross validation  \n\n```{r}\n# set random seed, for reproducibility \nset.seed(1234)\n# k-fold cross validation, with timing\nnround.cv = 200\nsystem.time( bst.cv <- xgb.cv(param=param, data=train.matrix, label=y, \n              nfold=4, nrounds=nround.cv, prediction=TRUE, verbose=FALSE) )\n```\n\nElapsed time is around 150 seconds (2.5 minutes).  \n\n```{r}\ntail(bst.cv$dt) \n```\n   \nFrom the cross validation, choose index with minimum multiclass error rate.  \nIndex will be used in the model training to fulfill expected minimum error rate of `< 1%`.  \n```{r}\n# index of minimum merror\nmin.merror.idx = which.min(bst.cv$dt[, test.merror.mean]) \nmin.merror.idx \n# minimum merror\nbst.cv$dt[min.merror.idx,]\n```\nBest cross-validation's minimum error rate `test.merror.mean` is around 0.006 (0.6%), happened at 106th iteration.   \n\n#### Confusion matrix \n\nTabulates the cross-validation's predictions of the model against the truths.  \n\n```{r}\n# get CV's prediction decoding\npred.cv = matrix(bst.cv$pred, nrow=length(bst.cv$pred)/num.class, ncol=num.class)\npred.cv = max.col(pred.cv, \"last\")\n# confusion matrix\nconfusionMatrix(factor(y+1), factor(pred.cv))\n```\n\nConfusion matrix shows concentration of correct predictions is on the diagonal, as expected.  \n  \nThe average accuracy is `99.38%`, with error rate is `0.62%`. So, expected error rate of less than `1%` is fulfilled.  \n\n#### Model training \n\nFit the XGBoost gradient boosting model on all of the training data.   \n```{r}\n# real model fit training, with full data\nsystem.time( bst <- xgboost(param=param, data=train.matrix, label=y, \n                           nrounds=min.merror.idx, verbose=0) )\n```\nTime elapsed is around 35 seconds.  \n\n#### Predicting the testing data\n\n```{r}\n# xgboost predict test data using the trained model\npred <- predict(bst, test.matrix)  \nhead(pred, 10)  \n```\n\n#### Post-processing\n\nOutput of prediction is the predicted probability of the 5 levels (columns) of outcome.  \nDecode the quantitative 5 levels of outcomes to qualitative letters (A, B, C, D, E).   \n  \n```{r}\n# decode prediction\npred = matrix(pred, nrow=num.class, ncol=length(pred)/num.class)\npred = t(pred)\npred = max.col(pred, \"last\")\npred.char = toupper(letters[pred])\n```\n\n(*The prediction result `pred.char` is not displayed intentionally due to Honour Code, because it is the answer of the \"project submission\" part.*)   \n\n#### Feature importance\n\n```{r fig.width=8, fig.height=12, dpi=72}\n# get the trained model\nmodel = xgb.dump(bst, with.stats=TRUE)\n# get the feature real names\nnames = dimnames(train.matrix)[[2]]\n# compute feature importance matrix\nimportance_matrix = xgb.importance(names, model=bst)\n\n# plot\ngp = xgb.plot.importance(importance_matrix)\nprint(gp) \n```\n\nFeature importance plot is useful to select only best features with highest correlation to the outcome(s). To improve model fitting performance (time or overfitting), less important features can be removed.   \n\n### Creating submission files \n\n```{r}\npath = \"./answer\"\npml_write_files = function(x) {\n    n = length(x)\n    for(i in 1: n) {\n        filename = paste0(\"problem_id_\", i, \".txt\")\n        write.table(x[i], file=file.path(path, filename), \n                    quote=FALSE, row.names=FALSE, col.names=FALSE)\n    }\n}\npml_write_files(pred.char)\n```\n------------------   ",
    "created" : 1479016700199.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4160245708",
    "id" : "9EB8B3D1",
    "lastKnownWriteTime" : 1479060090,
    "last_content_update" : 1479060090746,
    "path" : "~/Practical Machine Learning - Coursera/report.Rmd",
    "project_path" : "report.Rmd",
    "properties" : {
        "chunk_output_type" : "inline",
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}